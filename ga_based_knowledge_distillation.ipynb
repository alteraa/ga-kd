{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithms based Knowledge Distillation\n",
    "\n",
    "This notebook simply introduces a knowledge distillation application with a pretrained model built on Transformer architecture and uses genetics algorithms to isolate best fitted pretrained layers from base (teacher) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neural network libraries and utils\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import models, losses, evaluation\n",
    "from sentence_transformers import (\n",
    "    LoggingHandler,\n",
    "    SentenceTransformer,\n",
    "    util,\n",
    "    InputExample,\n",
    ")\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "# import general utils and helper libraries\n",
    "import random\n",
    "import typing as t\n",
    "from datetime import datetime\n",
    "# import genetics algorithm modules and utils\n",
    "import src.genetics.utils as ga_utils\n",
    "from src.genetics.population import Population\n",
    "from src.utils import (\n",
    "    download_dataset,\n",
    "    read_as_dataframe,\n",
    "    ALLNLI_DATASET_URL,\n",
    "    STS_BENCHMARK_DATASET_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters\n",
    "\n",
    "You can set all related parameters with this project, including genetic hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set utility variables\n",
    "random_state = random.Random(42) # random state to get same result for every run this notebook\n",
    "ga_utils.random_state = random_state # pass the same random state to genetics module as well\n",
    "# set global variables\n",
    "model_name = \"all-MiniLM-L12-v2\" # module to be distilled (ie. teacher model)\n",
    "output_path = f\"output/{model_name}_\" + datetime.now().strftime(r\"%Y-%m-%d_%H-%M-%S\") # output path to save model file and evaluation results\n",
    "max_train_samples = 1_000 # maximum number of training samples\n",
    "train_batch_size = 32 # batch size for training\n",
    "inference_batch_size = 32 # batch size for trained model\n",
    "max_sentence_length = 256 # maximum char length for each sample (sentence) in the training set\n",
    "### standard neural network hyperparameters ###\n",
    "epochs = 1 \n",
    "warmup_steps = 1000\n",
    "evaluation_steps = 5000\n",
    "learning_rate = 1e-4\n",
    "epsilon = 1e-6\n",
    "### standard neural network hyperparameters ###\n",
    "# set hyperparameters for genetic algorithms\n",
    "max_generation = 10 # maximum number of generations (ie. max iteration)\n",
    "population_size = 10 # population size (ie. number of chromosome for each generation)\n",
    "mutation_rate = 0.01 # mutation rate (%)\n",
    "chromosome_length = 10 # number of layers (because each gene represents a layer's indice)\n",
    "gene_value_range = (0, 12)  # value range for a gene => [a, b) means a is included while b is excluded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load teacher model\n",
    "\n",
    "Teacher model is the base model to be distilled. We will select its encoder layers within genetics processes to train best distilled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We will download the training and eval (ie. benchmark) datasets seperately and convert them to DataFrame.\n",
    "We are using these datasets:\n",
    "- For training: [**ALLNLI**](https://www.sbert.net/examples/datasets/README.html): Includes [SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) datsets\n",
    "- For benchmark: [**STS Benchmark**](http://ixa2.si.ehu.eus/stswiki/index.php/Main_Page): STS Benchmark comprises a selection of the English datasets used in the STS tasks organized in the context of SemEval between 2012 and 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_as_dataframe(url: str, download_path: str):\n",
    "    download_dataset(url, download_path)\n",
    "    return read_as_dataframe(download_path)\n",
    "\n",
    "\n",
    "# download training dataset (ALLNLI)\n",
    "training_ds = download_as_dataframe(\n",
    "    ALLNLI_DATASET_URL,\n",
    "    \"datasets/allnli.tsv.gz\",\n",
    ")\n",
    "# download evaluation (ie. benchmark) dataset (STSBENCHMARK)\n",
    "benchmark_ds = download_as_dataframe(\n",
    "    STS_BENCHMARK_DATASET_URL,\n",
    "    \"datasets/stsbenchmark.tsv.gz\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and benchmark evaluators\n",
    "\n",
    "Evaluators are proper objects to pass through fit function of teacher models. It includes the dataset and eval functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training evaluator\n",
    "train_sents = training_ds[training_ds[\"split\"] == \"train\"].loc[\n",
    "    :, [\"sentence1\", \"sentence2\"]\n",
    "]\n",
    "X_train = list(\n",
    "    set(train_sents[\"sentence1\"].to_list() + train_sents[\"sentence2\"].to_list())\n",
    ")\n",
    "random_state.shuffle(X_train)\n",
    "X_train = X_train[:max_train_samples]  # limit train dataset\n",
    "train_eval = evaluation.MSEEvaluator(\n",
    "    X_train,\n",
    "    X_train,\n",
    "    teacher_model,\n",
    "    name=\"allnli-train\",\n",
    ")\n",
    "\n",
    "# benchmark evaluator\n",
    "bench_sents = benchmark_ds[benchmark_ds[\"split\"] == \"dev\"].loc[\n",
    "    :, [\"sentence1\", \"sentence2\", \"score\"]\n",
    "]\n",
    "bench_samples = [\n",
    "    InputExample(\n",
    "        texts=[bench_sents.iloc[i, 0], bench_sents.iloc[i, 1]],\n",
    "        label=(float(bench_sents.iloc[i, 2]) / 5.0),\n",
    "    )\n",
    "    for i in range(len(bench_sents))\n",
    "]\n",
    "bench_eval = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "    bench_samples,\n",
    "    name=\"sts-dev\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate teacher model\n",
    "\n",
    "We first evaluate the teacher model on benchmark dataaset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_eval_result = bench_eval(teacher_model)\n",
    "print(\"Teacher model's benchmark result:\", teacher_eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define fitness function\n",
    "\n",
    "We define fitness function to pass genetics algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_student_model_from_layers(layers: t.List[int]) -> SentenceTransformer:\n",
    "    \"\"\"Create a student model same as teacher model with given layers indices.\n",
    "\n",
    "    Args:\n",
    "        layers (t.List[int]): List of layer indices.\n",
    "\n",
    "    Returns:\n",
    "        SentenceTransformer: Student model.\n",
    "    \"\"\"\n",
    "    student_model = SentenceTransformer(model_name)\n",
    "    auto_model = student_model._first_module().auto_model\n",
    "    new_layers = torch.nn.ModuleList(\n",
    "        [\n",
    "            layer_module\n",
    "            for i, layer_module in enumerate(auto_model.encoder.layer)\n",
    "            if i in layers\n",
    "        ]\n",
    "    )\n",
    "    auto_model.encoder.layer = new_layers\n",
    "    auto_model.config.num_hidden_layers = len(layers)\n",
    "    return student_model\n",
    "\n",
    "\n",
    "def fitness_function(layers: t.List[int]) -> float:\n",
    "    \"\"\"Fitness (or object) function. \n",
    "\n",
    "    Args:\n",
    "        layers (t.List[int]): List of layer indices.\n",
    "\n",
    "    Returns:\n",
    "        float: Benchmark eval result (rated with teacher model's result).\n",
    "    \"\"\"\n",
    "    student_model = get_student_model_from_layers(layers)\n",
    "    train_data = ParallelSentencesDataset(\n",
    "        student_model=student_model,\n",
    "        teacher_model=teacher_model,\n",
    "        batch_size=inference_batch_size,\n",
    "        use_embedding_cache=False,\n",
    "    )\n",
    "    train_data.add_dataset(\n",
    "        [[sent] for sent in X_train],\n",
    "        max_sentence_length=max_sentence_length,\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "    train_loss = losses.MSELoss(model=student_model)\n",
    "\n",
    "    student_model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        evaluator=evaluation.SequentialEvaluator([bench_eval, train_eval]),\n",
    "        epochs=epochs,\n",
    "        warmup_steps=warmup_steps,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        output_path=output_path,\n",
    "        optimizer_params={\"lr\": learning_rate, \"eps\": epsilon},\n",
    "        save_best_model=False,\n",
    "        use_amp=True,\n",
    "    )\n",
    "    return bench_eval(student_model) / teacher_eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run genetic algorithms\n",
    "\n",
    "Finally, we run the genetics processes to get best suited teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = Population(\n",
    "    population_size,\n",
    "    mutation_rate,\n",
    "    value_range=gene_value_range,\n",
    "    length=chromosome_length,\n",
    "    keep_best_chromosomes=True,\n",
    ")\n",
    "\n",
    "for i in range(max_generation):\n",
    "    population.eval(fitness_function)\n",
    "    population.update()\n",
    "    print(population.local_best, population.local_best.fitness)\n",
    "print(population.global_best, population.global_best.fitness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
